{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b16527",
   "metadata": {},
   "source": [
    "# I. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97ddb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import csv\n",
    "import math\n",
    "from math import log\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89354e85",
   "metadata": {},
   "source": [
    "# II. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77a1d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b900ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading node features...\n",
      "Loaded features for 3597 nodes\n",
      "Feature vector length: 932\n",
      "Building graph from training data...\n",
      "Graph built with 3597 nodes and 5248 edges\n",
      "Positive edges: 5248, Negative edges: 5248\n",
      "Loading test data...\n",
      "Loaded 3498 test pairs\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading node features...\")\n",
    "node_features = {}\n",
    "with open(\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        # ID, 932 features\n",
    "        node_id = int(float(row[0]))\n",
    "        features = np.array([float(x) for x in row[1:]])  # 932 features\n",
    "        node_features[node_id] = features\n",
    "\n",
    "print(f\"Loaded features for {len(node_features)} nodes\")\n",
    "print(f\"Feature vector length: {len(next(iter(node_features.values())))}\")\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for node_id in node_features:\n",
    "    G.add_node(node_id)\n",
    "\n",
    "print(\"Building graph from training data...\")\n",
    "edges_positive = []\n",
    "edges_negative = []\n",
    "with open(\"train.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 3:  # Ensure proper format\n",
    "            src, dst, label = map(int, parts)\n",
    "            if label == 1:\n",
    "                G.add_edge(src, dst)\n",
    "                edges_positive.append((src, dst))\n",
    "            else:\n",
    "                edges_negative.append((src, dst))\n",
    "\n",
    "print(f\"Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "print(f\"Positive edges: {len(edges_positive)}, Negative edges: {len(edges_negative)}\")\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_pairs = []\n",
    "with open(\"test.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_set = list(reader)\n",
    "test_pairs = [tuple(map(int, element[0].split())) for element in test_set]\n",
    "print(f\"Loaded {len(test_pairs)} test pairs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bc1a8",
   "metadata": {},
   "source": [
    "# II. Feature extraction function + custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63ff3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_adamic_adar_index(G, node1, node2):\n",
    "    \"\"\"Calculate Adamic-Adar index with proper error handling\"\"\"\n",
    "    common_neighbors = list(nx.common_neighbors(G, node1, node2))\n",
    "    if not common_neighbors:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    for w in common_neighbors:\n",
    "        degree = G.degree(w)\n",
    "        # Avoid division by zero when log(1) = 0\n",
    "        if degree > 1:\n",
    "            score += 1.0 / log(degree)\n",
    "    return score\n",
    "\n",
    "def custom_resource_allocation_index(G, node1, node2):\n",
    "    \"\"\"Calculate Resource Allocation index with proper error handling\"\"\"\n",
    "    common_neighbors = list(nx.common_neighbors(G, node1, node2))\n",
    "    if not common_neighbors:\n",
    "        return 0\n",
    "    \n",
    "    score = 0\n",
    "    for w in common_neighbors:\n",
    "        degree = G.degree(w)\n",
    "        if degree > 0:  # Avoid division by zero\n",
    "            score += 1.0 / degree\n",
    "    return score\n",
    "\n",
    "def custom_jaccard_coefficient(G, node1, node2):\n",
    "    \"\"\"Calculate Jaccard coefficient with proper error handling\"\"\"\n",
    "    neighbors1 = set(G.neighbors(node1))\n",
    "    neighbors2 = set(G.neighbors(node2))\n",
    "    \n",
    "    # Handle edge cases\n",
    "    union_size = len(neighbors1.union(neighbors2))\n",
    "    if union_size == 0:\n",
    "        return 0\n",
    "    \n",
    "    return len(neighbors1.intersection(neighbors2)) / union_size\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors with error handling\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 * norm2 > 0:\n",
    "        return dot_product / (norm1 * norm2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbb68328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(G, node1, node2):\n",
    "    \"\"\"Extract features for a node pair using NetworkX graph with robust error handling\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Check if both nodes exist in the graph\n",
    "    if node1 not in G or node2 not in G:\n",
    "        # Return default features if nodes don't exist\n",
    "        return [0] * 10  # Adjusted for feature count\n",
    "    \n",
    "    # Common neighbors\n",
    "    common_neighbors = list(nx.common_neighbors(G, node1, node2))\n",
    "    features.append(len(common_neighbors))\n",
    "    \n",
    "    # Jaccard coefficient\n",
    "    features.append(custom_jaccard_coefficient(G, node1, node2))\n",
    "    \n",
    "    # Preferential attachment\n",
    "    features.append(G.degree(node1) * G.degree(node2))\n",
    "    \n",
    "    # Adamic-Adar index\n",
    "    features.append(custom_adamic_adar_index(G, node1, node2))\n",
    "    \n",
    "    # Resource allocation index\n",
    "    features.append(custom_resource_allocation_index(G, node1, node2))\n",
    "    \n",
    "    # Shortest path length\n",
    "    try:\n",
    "        if G.has_edge(node1, node2):\n",
    "            G.remove_edge(node1, node2)\n",
    "            path_length = nx.shortest_path_length(G, node1, node2)\n",
    "            G.add_edge(node1, node2)  # Restore the edge\n",
    "        else:\n",
    "            path_length = nx.shortest_path_length(G, node1, node2)\n",
    "        features.append(path_length)\n",
    "    except (nx.NetworkXNoPath, nx.NetworkXError):\n",
    "        features.append(-1)  # No path exists\n",
    "    \n",
    "    # Node feature similarity\n",
    "    if node1 in node_features and node2 in node_features:\n",
    "        # Cosine similarity between node feature vectors\n",
    "        cos_sim = cosine_similarity(node_features[node1], node_features[node2])\n",
    "        features.append(cos_sim)\n",
    "        \n",
    "        # Correlation between node feature vectors\n",
    "        try:\n",
    "            corr = np.corrcoef(node_features[node1], node_features[node2])[0, 1]\n",
    "            features.append(corr if not np.isnan(corr) else 0)\n",
    "        except:\n",
    "            features.append(0)\n",
    "        \n",
    "        # Number of matching non-zero features (indicating shared topics)\n",
    "        n1_nonzero = node_features[node1] != 0\n",
    "        n2_nonzero = node_features[node2] != 0\n",
    "        matching_topics = np.logical_and(n1_nonzero, n2_nonzero).sum()\n",
    "        features.append(matching_topics)\n",
    "    else:\n",
    "        # Default values if node features not available\n",
    "        features.extend([0, 0, 0])\n",
    "    \n",
    "    # Community detection feature (shared neighbors ratio)\n",
    "    try:\n",
    "        neighbors1 = set(G.neighbors(node1))\n",
    "        neighbors2 = set(G.neighbors(node2))\n",
    "        if len(neighbors1) + len(neighbors2) > 0:\n",
    "            shared_neighbors_ratio = len(neighbors1.intersection(neighbors2)) / (len(neighbors1) + len(neighbors2) - len(neighbors1.intersection(neighbors2)))\n",
    "        else:\n",
    "            shared_neighbors_ratio = 0\n",
    "        features.append(shared_neighbors_ratio)\n",
    "    except:\n",
    "        features.append(0)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7849240",
   "metadata": {},
   "source": [
    "# IV. Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f048db9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training data...\n",
      "Using 5248 examples from each class for balanced training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing positive examples: 100%|██████████| 5248/5248 [00:00<00:00, 8789.01it/s]\n",
      "Processing negative examples: 100%|██████████| 5248/5248 [00:00<00:00, 9217.74it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting features for training data...\")\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Balance the dataset to avoid bias\n",
    "min_examples = min(len(edges_positive), len(edges_negative))\n",
    "print(f\"Using {min_examples} examples from each class for balanced training\")\n",
    "\n",
    "# Positive examples\n",
    "for src, dst in tqdm(edges_positive[:min_examples], desc=\"Processing positive examples\"):\n",
    "    features = extract_features(G, src, dst)\n",
    "    X.append(features)\n",
    "    y.append(1)\n",
    "\n",
    "# Negative examples\n",
    "for src, dst in tqdm(edges_negative[:min_examples], desc=\"Processing negative examples\"):\n",
    "    features = extract_features(G, src, dst)\n",
    "    X.append(features)\n",
    "    y.append(0)\n",
    "\n",
    "# Split for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6455d2",
   "metadata": {},
   "source": [
    "# V. Train & performance training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a74559b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training RandomForest: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "with tqdm(total=1, desc=\"Training RandomForest\") as pbar:\n",
    "    model.fit(X_train, y_train)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eae1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Validation Results -----\n",
      "Validation Accuracy: 0.9210\n",
      "Validation F1 Score: 0.9251\n",
      "Validation Precision: 0.8791\n",
      "Validation Recall: 0.9762\n",
      "Validation Confusion Matrix:\n",
      "[[ 909  141]\n",
      " [  25 1025]]\n"
     ]
    }
   ],
   "source": [
    "y_val_pred = model.predict(X_val)\n",
    "print(\"\\n----- Validation Results -----\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Validation F1 Score: {f1_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Validation Precision: {precision_score(y_val, y_val_pred):.4f}\")\n",
    "print(f\"Validation Recall: {recall_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"Validation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "376c3a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Feature Importance -----\n",
      "Preferential Attachment: 0.7889\n",
      "Shortest Path Length: 0.1360\n",
      "Jaccard Coefficient: 0.0184\n",
      "Node Feature Correlation: 0.0181\n",
      "Shared Neighbors Ratio: 0.0098\n",
      "Node Feature Cosine Similarity: 0.0086\n",
      "Adamic-Adar Index: 0.0081\n",
      "Resource Allocation Index: 0.0068\n",
      "Shared Topics Count: 0.0032\n",
      "Common Neighbors: 0.0021\n"
     ]
    }
   ],
   "source": [
    "# Feature importance\n",
    "feature_names = [\n",
    "    \"Common Neighbors\", \n",
    "    \"Jaccard Coefficient\", \n",
    "    \"Preferential Attachment\", \n",
    "    \"Adamic-Adar Index\",\n",
    "    \"Resource Allocation Index\", \n",
    "    \"Shortest Path Length\", \n",
    "    \"Node Feature Cosine Similarity\",\n",
    "    \"Node Feature Correlation\",\n",
    "    \"Shared Topics Count\",\n",
    "    \"Shared Neighbors Ratio\"\n",
    "]\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"\\n----- Feature Importance -----\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d943c2fb",
   "metadata": {},
   "source": [
    "# Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6dc4b0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test pairs: 100%|██████████| 3498/3498 [00:00<00:00, 7637.97it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating predictions for test data...\")\n",
    "test_features = []\n",
    "for src, dst in tqdm(test_pairs, desc=\"Processing test pairs\"):\n",
    "    test_features.append(extract_features(G, src, dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1eced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running prediction on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 1/1 [00:00<00:00, 17.91it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Running prediction on test data...\")\n",
    "with tqdm(total=1, desc=\"Predicting\") as pbar:\n",
    "    test_predictions = model.predict(test_features)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35e967f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Test Results -----\n",
      "Total test samples: 3498\n",
      "Predicted positives (1): 1173\n",
      "Predicted negatives (0): 2325\n",
      "Positive rate: 0.3353\n",
      "\n",
      "Random baseline positive rate would be: 0.5000\n",
      "Our model differs from random by: 0.1647\n",
      "\n",
      "Predictions saved to predictions.csv\n",
      "\n",
      "Total runtime: 2.91 seconds\n",
      "\n",
      "----- Summary -----\n",
      "Our feature-based model uses:\n",
      "1. Graph structure (common neighbors, path length, etc.)\n",
      "2. Node features from Wikipedia (932 text features per actor)\n",
      "Performance on validation set: 0.9210 accuracy\n",
      "Expected improvement over random baseline: ~0.4210\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n----- Test Results -----\")\n",
    "print(f\"Total test samples: {len(test_predictions)}\")\n",
    "print(f\"Predicted positives (1): {np.sum(test_predictions == 1)}\")\n",
    "print(f\"Predicted negatives (0): {np.sum(test_predictions == 0)}\")\n",
    "print(f\"Positive rate: {np.mean(test_predictions):.4f}\")\n",
    "\n",
    "# Compare with random baseline\n",
    "random_baseline_prob = 0.5\n",
    "print(f\"\\nRandom baseline positive rate would be: {random_baseline_prob:.4f}\")\n",
    "print(f\"Our model differs from random by: {abs(np.mean(test_predictions) - random_baseline_prob):.4f}\")\n",
    "\n",
    "with open(\"predictions.csv\", \"w\") as pred_file:\n",
    "    csv_out = csv.writer(pred_file)\n",
    "    csv_out.writerow([\"ID\", \"Predicted\"])\n",
    "    for i, prediction in enumerate(test_predictions):\n",
    "        csv_out.writerow([i, int(prediction)])\n",
    "\n",
    "print(\"\\nPredictions saved to predictions.csv\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTotal runtime: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\n----- Summary -----\")\n",
    "print(\"Our feature-based model uses:\")\n",
    "print(\"1. Graph structure (common neighbors, path length, etc.)\")\n",
    "print(\"2. Node features from Wikipedia (932 text features per actor)\")\n",
    "print(f\"Performance on validation set: {accuracy_score(y_val, y_val_pred):.4f} accuracy\")\n",
    "print(f\"Expected improvement over random baseline: ~{(accuracy_score(y_val, y_val_pred) - 0.5):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
